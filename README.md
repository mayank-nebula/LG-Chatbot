def process_question(question, chatHistory, permissions, filters, stores, image, llm):
    """Process a question and return the answer"""
    global user_permissions, sources
    user_permissions = permissions.copy()
    sources.clear()

    if not chatHistory:
        title = create_new_title(question, llm)
    else:
        title = ""

    if general_chat(question):
        response = handle_salutation(
            chatHistory=chatHistory, llm=llm, question=question
        )
        return response, {"This response is generated by ChatGPT": ""}, title

    if chatHistory:
        question = standalone_question(question, chatHistory, llm)

    if filters:
        retriever = create_retriever(filters, stores)
    else:
        if stores == "GPT":
            retriever = retriever_gpt
            increment_request_count()
        else:
            retriever = retriever_ollama

    if image == "Yes":
        if llm == "GPT":
            llm_to_use = llm_gpt
        else:
            llm_to_use = ChatOllama(
                temperature=0,
                model=llava_llama3,
                base_url=base_url,
            )
    else:
        if llm == "GPT":
            llm_to_use = llm_gpt
        else:
            llm_to_use = ChatOllama(temperature=0, model=llama3, base_url=base_url)

from flask import Flask, request, jsonify
from flask_cors import CORS

from processing_multi_vector_retriever import process_question

app = Flask(__name__)
CORS(app)


@app.route("/flask", methods=["POST"])
def process():
    data = request.get_json()
    question = data.get("question")
    chatHistory = data.get("chat_history", [])
    permissions = data.get("userPermissions", [])
    filters = data.get("filters", [])
    stores = data.get("stores", "GPT")
    image = data.get("image", "No")
    llm = data.get("llm", "GPT")

    if not question:
        return jsonify({"error": "No question provided"}), 400

    try:
        response, sources, title = process_question(
            question, chatHistory, permissions, filters, stores, image, llm
        )

        return jsonify({"response": response, "sources":sources, "title":title})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)


    chain = multi_modal_rag_chain_source(retriever, llm_to_use, llm, image, filters)

    response = chain.invoke(question)

    return response, sources, title
