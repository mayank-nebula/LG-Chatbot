def process_question(question, chatHistory, permissions, filters, stores, image, llm):
    """Process a question and return the answer"""
    global user_permissions, sources
    user_permissions = permissions.copy()
    sources.clear()

    if not chatHistory:
        title = create_new_title(question, llm)
    else:
        title = ""

    if general_chat(question):
        response = handle_salutation(
            chatHistory=chatHistory, llm=llm, question=question
        )
        return response, {"This response is generated by ChatGPT": ""}, title

    if chatHistory:
        question = standalone_question(question, chatHistory, llm)

    if filters:
        retriever = create_retriever(filters, stores)
    else:
        if stores == "GPT":
            retriever = retriever_gpt
            increment_request_count()
        else:
            retriever = retriever_ollama

    if image == "Yes":
        if llm == "GPT":
            llm_to_use = llm_gpt
        else:
            llm_to_use = ChatOllama(
                temperature=0,
                model=llava_llama3,
                base_url=base_url,
            )
    else:
        if llm == "GPT":
            llm_to_use = llm_gpt
        else:
            llm_to_use = ChatOllama(temperature=0, model=llama3, base_url=base_url)

    chain = multi_modal_rag_chain_source(retriever, llm_to_use, llm, image, filters)

    response = chain.invoke(question)

    return response, sources, title
